from flask import Flask, render_template, request, jsonify
import os
import re

from utils.interview_sources import get_interview_metadata


app = Flask(__name__, static_folder="static", template_folder="templates")

# ğŸš€ å†…å­˜ç¼“å­˜ - æ€§èƒ½ä¼˜åŒ–
ENABLE_CACHE = os.environ.get("ENABLE_CACHE", "true").lower() == "true"
manga_text_cache = {}  # æ¼«ç”»æ–‡æœ¬ç¼“å­˜
interview_text_cache = {}  # è®¿è°ˆæ–‡æœ¬ç¼“å­˜

# ğŸš€ ç¼“å­˜åˆå§‹åŒ–å‡½æ•°
def init_manga_cache():
    """åˆå§‹åŒ–æ¼«ç”»æ–‡æœ¬ç¼“å­˜"""
    if not ENABLE_CACHE or manga_text_cache:
        return
    
    base_dir = "data/japanese_text"
    if not os.path.exists(base_dir):
        return
    
    print("ğŸ“¥ æ­£åœ¨é¢„åŠ è½½æ¼«ç”»æ–‡æœ¬åˆ°å†…å­˜ç¼“å­˜...")
    for filename in os.listdir(base_dir):
        if filename.endswith(".txt"):
            file_path = os.path.join(base_dir, filename)
            try:
                with open(file_path, encoding="utf-8") as f:
                    manga_text_cache[filename] = f.read()
            except Exception as e:
                print(f"âŒ ç¼“å­˜æ–‡ä»¶å¤±è´¥ {filename}: {e}")
    
    print(f"âœ… å·²ç¼“å­˜ {len(manga_text_cache)} ä¸ªæ¼«ç”»æ–‡ä»¶")

# åŠŸèƒ½ä¸€ï¼šæ¼«ç”»æ–‡æœ¬æ£€ç´¢ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
def count_word_in_documents(word):
    result = []
    base_dir = "data/japanese_text"
    
    # ğŸš€ ä½¿ç”¨ç¼“å­˜æˆ–ç›´æ¥è¯»å–æ–‡ä»¶
    if ENABLE_CACHE:
        init_manga_cache()  # æ‡’åŠ è½½
        file_data = manga_text_cache
    else:
        # åŸå§‹æ–¹å¼ï¼šç›´æ¥è¯»å–æ–‡ä»¶
        file_data = {}
        if not os.path.exists(base_dir):
            return result
        
        for filename in os.listdir(base_dir):
            if filename.endswith(".txt"):
                file_path = os.path.join(base_dir, filename)
                with open(file_path, encoding="utf-8") as f:
                    file_data[filename] = f.read()
    
    # å¤„ç†é€»è¾‘ä¿æŒä¸å˜
    for filename, text in file_data.items():
        # åˆ†é¡µç»“æ„ï¼š===Page X===
        pages = re.split(r"===Page (\d+)===", text)
        page_nums = []
        total_count = 0

        # ç»“æ„ï¼š['', page_num1, content1, page_num2, content2, ...]
        for i in range(1, len(pages) - 1, 2):
            page_number = int(pages[i])
            content = pages[i + 1]
            count = content.count(word)
            if count > 0:
                page_nums.append(page_number)
                total_count += count

        if total_count > 0:
            volume_match = re.match(r"^(\d+)\.txt$", filename)
            volume = int(volume_match.group(1)) if volume_match else filename
            result.append({
                "volume": volume,
                "count": total_count,
                "pages": sorted(page_nums)
            })

    # âœ… æŒ‰å·å·æ’åº
    result.sort(key=lambda x: x["volume"])
    return result

# é¦–é¡µï¼šè¿”å› HTML é¡µé¢
@app.route("/")
def home():
    return render_template("index.html")

# æ¼«ç”»æ–‡æœ¬æ£€ç´¢æ¥å£
@app.route("/search", methods=["POST"])
def search():
    word = request.form.get("word", "").strip()
    result = count_word_in_documents(word)
    return jsonify(result)

# ğŸš€ è®¿è°ˆç¼“å­˜åˆå§‹åŒ–å‡½æ•°
def init_interview_cache():
    """åˆå§‹åŒ–è®¿è°ˆæ–‡æœ¬ç¼“å­˜"""
    if not ENABLE_CACHE or interview_text_cache:
        return
    
    base_dir = "data/interviews"
    if not os.path.exists(base_dir):
        return
    
    print("ğŸ“¥ æ­£åœ¨é¢„åŠ è½½è®¿è°ˆæ–‡æœ¬åˆ°å†…å­˜ç¼“å­˜...")
    for root, dirs, files in os.walk(base_dir):
        for filename in files:
            if filename.endswith(".txt"):
                filepath = os.path.join(root, filename)
                rel_path = os.path.relpath(filepath, base_dir)
                try:
                    with open(filepath, encoding="utf-8") as f:
                        interview_text_cache[rel_path] = f.read()
                except Exception as e:
                    print(f"âŒ ç¼“å­˜è®¿è°ˆæ–‡ä»¶å¤±è´¥ {rel_path}: {e}")
    
    print(f"âœ… å·²ç¼“å­˜ {len(interview_text_cache)} ä¸ªè®¿è°ˆæ–‡ä»¶")

# è®¿è°ˆèµ„æ–™æ¥å£ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
@app.route("/interview_search", methods=["POST"])
def interview_search():
    word = request.form.get("word", "").strip()
    base_dir = "data/interviews"
    results = []

    if not word:
        return jsonify(results)

    # ğŸš€ ä½¿ç”¨ç¼“å­˜æˆ–ç›´æ¥è¯»å–æ–‡ä»¶
    if ENABLE_CACHE:
        init_interview_cache()  # æ‡’åŠ è½½
        file_data = interview_text_cache
    else:
        # åŸå§‹æ–¹å¼ï¼šç›´æ¥è¯»å–æ–‡ä»¶
        file_data = {}
        for root, dirs, files in os.walk(base_dir):
            for filename in files:
                if filename.endswith(".txt"):
                    filepath = os.path.join(root, filename)
                    rel_path = os.path.relpath(filepath, base_dir)
                    try:
                        with open(filepath, encoding="utf-8") as f:
                            file_data[rel_path] = f.read()
                    except Exception as e:
                        print(f"âŒ Error reading {filepath}: {e}")

    # å¤„ç†é€»è¾‘ä¿æŒä¸å˜
    for rel_path, text in file_data.items():
        try:
            count = text.count(word)
            if count > 0:
                # åŒ¹é…å¥å­ç‰‡æ®µï¼ˆç®€åŒ–å¤„ç†ï¼šç”¨å¥å·ã€æ¢è¡Œã€é—®å·ã€å¹å·åˆ†å¥ï¼‰
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
                matched_snippets = []
                for s in sentences:
                    if word in s:
                        snippet = f"...{s.strip()}..."
                        matched_snippets.append(snippet)
                matched_snippets = matched_snippets[:3]  # æœ€å¤š 3 æ¡

                # æ¥æºä¿¡æ¯ï¼šå¯ä»¥ç»§ç»­æ‰©å±•æ›´å¤šè§„åˆ™
                meta = get_interview_metadata(rel_path)

                results.append({
                    "file": rel_path,
                    "count": count,
                    "source": meta["source"],
                    "url": meta["url"],
                    "snippets": matched_snippets
                })
        except Exception as e:
            print(f"âŒ Error processing {rel_path}: {e}")

    results.sort(key=lambda x: -x["count"])
    return jsonify(results)


# ğŸš€ ç¼“å­˜çŠ¶æ€æŸ¥çœ‹æ¥å£ï¼ˆè°ƒè¯•ç”¨ï¼‰
@app.route("/cache_status")
def cache_status():
    """æŸ¥çœ‹ç¼“å­˜çŠ¶æ€"""
    status = {
        "cache_enabled": ENABLE_CACHE,
        "manga_cache_size": len(manga_text_cache),
        "interview_cache_size": len(interview_text_cache),
        "total_memory_usage_mb": sum(len(text.encode('utf-8')) for text in manga_text_cache.values()) / 1024 / 1024 +
                                sum(len(text.encode('utf-8')) for text in interview_text_cache.values()) / 1024 / 1024
    }
    return jsonify(status)

# æœªæ¥ï¼šæ¥å…¥ LLM é—®ç­”æ¥å£
@app.route("/ask", methods=["POST"])
def ask():
    question = request.form.get("question", "")
    return jsonify({"answer": f"æš‚æœªæ¥å…¥ LLMï¼Œæ”¶åˆ°é—®é¢˜ï¼š{question}"})

# å¯åŠ¨æœåŠ¡ï¼ˆé€‚é… Render çš„ PORT ç¯å¢ƒå˜é‡ï¼‰
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 7860))
    app.run(host="0.0.0.0", port=port)
