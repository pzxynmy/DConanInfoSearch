"""
ç”¨äºçˆ¬å–UPä¸»å¤§ç†å¯ºå°‘å¿çš„åæŸ¯è®¿è°ˆç›¸å…³ä¸“æ æ–‡ç« 

ç›®å‰å­˜åœ¨çˆ¬å–æ—¶æ¼çˆ¬ä½†ä¸æŠ¥é”™çš„é—®é¢˜
"""

import logging
import os
import re
import json
import requests
import time
from bs4 import BeautifulSoup
from datetime import datetime

# è‡ªåŠ¨ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆä¸è„šæœ¬åŒåï¼‰
try:
    script_name = os.path.splitext(os.path.basename(__file__))[0]
except NameError:
    script_name = "interviews_from_bilibili_article"

log_path = f"./logs/{script_name}.log"

# âœ… ç¡®ä¿ logs ç›®å½•å­˜åœ¨
os.makedirs(os.path.dirname(log_path), exist_ok=True)

# è®¾ç½® logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(message)s",
    handlers=[
        logging.FileHandler(log_path, mode="a", encoding="utf-8"),
        # logging.StreamHandler()  # å¦‚æœéœ€è¦ç»ˆç«¯è¾“å‡ºï¼Œå–æ¶ˆæ³¨é‡Š
    ]
)

# âœ… åœ¨æ—¥å¿—æ–‡ä»¶ä¸­å†™å…¥æ—¶é—´åˆ†éš”çº¿
logging.info("\n" + "="*20 + f" Script started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} " + "="*20 + "\n")

logging.info("âœ… æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")


# readlist_ids = [
#     725889, #å¤§ç†å¯ºå°‘å¿-æ—¥å¸¸æŸ¯å—é‡‡è®¿ç‰©æ–™
#     780064, #å¤§ç†å¯ºå°‘å¿-æŸ¯å—è¿è½½30å‘¨å¹´ é’å±±åˆšæ˜Œxä¸œé‡åœ­å¾ç‰¹åˆ«å¯¹è°ˆ  
#     806967, #å¤§ç†å¯ºå°‘å¿-M27ç›¸å…³ä¿¡æ¯åŠé‡‡è®¿
#     748494, #å¤§ç†å¯ºå°‘å¿-M26ç›¸å…³ä¿¡æ¯åŠé‡‡è®¿
#     922184, #å¤§ç†å¯ºå°‘å¿-M28ç›¸å…³ä¿¡æ¯åŠé‡‡è®¿     
#     432168, #æŸ¯ç ”æ‰€ç¿»è¯‘   
#                 ]  # â† å¯æŒç»­æ›´æ–°
with open("./data/interviews/bilibili_article/bilibili_readlists.json", encoding="utf-8") as f:
    readlist_map = json.load(f)

readlist_ids = list(readlist_map.keys())  # ['725889', '780064', ...]

headers = {"User-Agent": "Mozilla/5.0"}

# è·å–åˆé›†ä¸­çš„æ–‡ç« ç®€ç•¥ä¿¡æ¯
def get_article_list(readlist_id):
    url = f"https://api.bilibili.com/x/article/list/web/articles?id={readlist_id}&pn=1&ps=100"
    resp = requests.get(url, headers=headers)
    return resp.json()['data']['articles']

# è·å–æ–‡ç« æ­£æ–‡ï¼ˆæ–¹å¼ä¸€ï¼šè§£æç½‘é¡µ HTMLï¼‰
def get_article_from_web(article_id):
    url = f"https://www.bilibili.com/read/cv{article_id}"
    resp = requests.get(url, headers=headers)
    soup = BeautifulSoup(resp.text, "html.parser")

    title = soup.find("h1").text.strip()
    content_div = soup.find("div", class_="article-content")
    content = content_div.get_text(separator="\n", strip=True) if content_div else "æ­£æ–‡è·å–å¤±è´¥"
    return title, content

# # è·å–æ–‡ç« æ­£æ–‡ï¼ˆæ–¹å¼äºŒï¼šè°ƒç”¨ APIï¼‰
def get_article_from_api(article_id):
    url = f"https://api.bilibili.com/x/article/view?id={article_id}"
    resp = requests.get(url, headers=headers)
    data = resp.json()['data']
    title = data['title']
    html = data['content']  # HTML æ­£æ–‡
    # çº¯æ–‡æœ¬æå–ï¼ˆä¹Ÿå¯ç”¨ html2text ç­‰å¤„ç†æˆ markdownï¼‰
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text(separator="\n", strip=True)
    return title, text

# ä¿å­˜

for readlist_id in readlist_ids:
    logging.info(f"\nğŸ“š å¼€å§‹å¤„ç†åˆé›† rl{readlist_id} ...")
    try:
        article_list = get_article_list(readlist_id)
    except Exception as e:
        logging.info(f"âŒ åˆé›† rl{readlist_id} è·å–å¤±è´¥ï¼š{e}")
        continue

    logging.info(f"âœ… åˆé›† rl{readlist_id} å…± {len(article_list)} ç¯‡æ–‡ç« ")

    output_dir = f"./data/interviews/bilibili_article/rl{readlist_id}/"
    os.makedirs(output_dir, exist_ok=True)

    for idx, article in enumerate(article_list, 1):
        article_id = article.get('id')
        if not article_id:
            logging.info(f"âš ï¸ è·³è¿‡ç¬¬ {idx} é¡¹ï¼šæ— æœ‰æ•ˆ ID")
            continue

        try:
            title, content = get_article_from_api(article_id)

            if not title or not content:
                raise ValueError("ç¼ºå¤±æ ‡é¢˜æˆ–æ­£æ–‡")

            safe_title = title[:30].replace("/", "-").replace("\\", "-")
            filename = os.path.join(output_dir, f"{idx:02d}_{safe_title}.txt")

            with open(filename, "w", encoding="utf-8") as f:
                f.write(f"Title: {title}\nURL: https://www.bilibili.com/read/cv{article_id}\n\n{content}")

            logging.info(f"âœ… å·²ä¿å­˜ï¼š{filename}")
        except Exception as e:
            logging.info(f"âŒ è·å–å¤±è´¥ï¼šcv{article_id}ï¼ŒåŸå› ï¼š{e}")
