#!/usr/bin/env python3
"""
ç™¾åº¦è´´å§å¸–å­å†…å®¹çˆ¬è™« + æ™ºèƒ½è¿‡æ»¤å™¨
æŠ“å–æŒ‡å®šå¸–å­çš„æ‰€æœ‰æ¥¼å±‚å†…å®¹å¹¶è‡ªåŠ¨è¿‡æ»¤å‡ºé«˜ä»·å€¼é—®ç­”
URL: https://tieba.baidu.com/p/7223841891?pn=1-26
"""

import random
import requests
import time
import os
import re
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
import json

class TiebaQAFilter:
    """è´´å§é—®ç­”å†…å®¹æ™ºèƒ½è¿‡æ»¤å™¨"""
    def __init__(self):
        # éœ€è¦è¿‡æ»¤æ‰çš„å†…å®¹
        self.fold_keywords = [
            "è¯¥æ¥¼å±‚ç–‘ä¼¼è¿è§„å·²è¢«ç³»ç»ŸæŠ˜å ",
            "æ­¤å›å¤å·²è¢«æŠ˜å ", 
            "æŠ˜å å›å¤",
            "éšè—æ­¤æ¥¼",
            "æŸ¥çœ‹æ­¤æ¥¼"
        ]
        
        # æ— æ„ä¹‰çš„çŸ­å›å¤
        self.meaningless_patterns = [
            r"^[é¡¶æ”¯æŒæ²™å‘å‰æ’å æ¥¼]+$",
            r"^[0-9]+æ¥¼$",
            r"^[å“ˆå“¦å—¯é¢å’¦å‘µ]+$",
            r"^[è‰666ç¬‘å“­]+$",
            r"^[\.ã€‚ï¼ï¼Ÿ\s]+$",
            r"^æ”¶è—äº†?$",
            r"^é©¬å…‹$",
            r"^mark$",
            r"^[+1åŒ]+$"
        ]
        
        # é—®é¢˜ç›¸å…³å…³é”®è¯ï¼ˆé«˜ä¼˜å…ˆçº§ä¿ç•™ï¼‰
        self.question_keywords = [
            "é—®", "è¯·é—®", "æƒ³é—®", "æ±‚é—®", "å’¨è¯¢", "ç–‘é—®",
            "ï¼Ÿ", "å—", "å‘¢", "å§", "æ±‚åŠ©", "help",
            "æœ‰æ²¡æœ‰", "æ˜¯å¦", "ä¼šä¸ä¼š", "èƒ½ä¸èƒ½",
            "å“ªé‡Œ", "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¦‚ä½•"
        ]
        
        # å›ç­”ç›¸å…³å…³é”®è¯
        self.answer_keywords = [
            "ç­”", "å›ç­”", "è§£é‡Š", "è¯´æ˜", "åˆ†æ", "æ¨ç†",
            "æ˜¯è¿™æ ·", "æ®æˆ‘æ‰€çŸ¥", "åº”è¯¥æ˜¯", "å…¶å®",
            "æ ¹æ®", "å‡ºå¤„", "æ¥æº", "è®¿è°ˆä¸­", "é’å±±è¯´"
        ]
        
        self.conan_keywords = [
            # å…³é”®è¯
            "æŸ¯å—", "æ–°ä¸€", "å·¥è—¤", "ç°åŸ", "å“€", "å°å“€", "æ¯›åˆ©", "å…°", "å°å…°",
            "é’å±±", "73", "Boss", "é»‘è¡£ç»„ç»‡", "APTX",
            "å‰§æƒ…", "çœŸç›¸", "æ¨ç†", "æ¡ˆä»¶", "é›†æ•°", "æ¼«ç”»",
            "åŠ¨ç”»", "è®¿è°ˆ", "å®˜æ–¹", "ä½œè€…",
            
            # ä¸»è¦è§’è‰²
            "æ±Ÿæˆ·å·", "å®«é‡", "å¿—ä¿", "å°äº”éƒ", "åšå£«", "é˜¿ç¬ ",
            
            # å°‘å¹´ä¾¦æ¢å›¢
            "æ­¥ç¾", "å…‰å½¦", "å…ƒå¤ª", "ä¾¦æ¢å›¢",
            
            # é‡è¦é…è§’
            "æœéƒ¨", "å¹³æ¬¡", "å’Œå¶", "è¿œå±±", "åŸºå¾·", "æ€ªç›—", "é»‘ç¾½", "å¿«æ–—",
            "å›­å­", "é“ƒæœ¨", "å¦ƒè‹±ç†", "æœ‰å¸Œå­", "ä¼˜ä½œ",
            
            # FBI/CIA/å…¬å®‰
            "èµ¤äº•", "ç§€ä¸€", "å®‰å®¤", "é€", "é™è°·", "é›¶", "æ°´æ— ", "æ€œå¥ˆ", 
            "æœ¬å ‚", "ç‘›æµ·", "å†²çŸ¢", "æ˜´", "ä¸–è‰¯", "çœŸçº¯",
            
            # é»‘è¡£ç»„ç»‡æˆå‘˜
            "ç´é…’", "ä¼ç‰¹åŠ ", "è‹¦è‰¾é…’", "è´å°”æ‘©å¾·", "è˜­å§†", "çƒä¸¸", "è“®è€¶",
            "æ³¢æœ¬", "åŸºå°”", "é»‘éº¦", "é¾™èˆŒå…°", "çš®æ–¯å…‹",
            
            # è­¦å¯Ÿç›¸å…³
            "ç›®æš®", "é«˜æœ¨", "ä½è—¤", "ç¾å’Œå­", "ç™½é¸Ÿ", "æ¾æœ¬", "èŒ¶æœ¨",
            
            # ç»„ç»‡ä»£å·
            "gin", "vodka", "vermouth", "rum", "bourbon", "kir", "rye",
            
            # å…¶ä»–é‡è¦äººç‰©
            "å°æ—", "æ¾„å­", "æ¨ªæ²Ÿ", "å±±æ‘", "å¤§å’Œ", "ä¸ŠåŸ"
        ]

        
        # æœ‰ç”¨ä¿¡æ¯å…³é”®è¯
        self.useful_keywords = [
            "è§„åˆ™", "é“¾æ¥", "æ•´ç†", "èµ„æ–™", "æ¡£æ¡ˆ",
            "å‡ºå¤„", "æ¥æº", "ç¿»è¯‘", "ç¡®è®¤", "å¦è®¤"
        ]
    
    def is_system_folded(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¢«ç³»ç»ŸæŠ˜å """
        for keyword in self.fold_keywords:
            if keyword in content:
                return True
        return False
    
    def is_meaningless_short(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æ— æ„ä¹‰çš„çŸ­å›å¤"""
        content = content.strip()
        
        # é•¿åº¦è¿‡çŸ­
        if len(content) < 5:
            return True
        
        # åŒ¹é…æ— æ„ä¹‰æ¨¡å¼
        for pattern in self.meaningless_patterns:
            if re.match(pattern, content, re.IGNORECASE):
                return True
        
        return False
    
    def calculate_content_score(self, content: str) -> float:
        """è®¡ç®—å†…å®¹ä»·å€¼åˆ†æ•°"""
        score = 0.0
        
        # åŸºç¡€åˆ†æ•°ï¼ˆæ ¹æ®é•¿åº¦ï¼‰
        if len(content) > 50:
            score += 1.0
        elif len(content) > 20:
            score += 0.5
        
        # é—®é¢˜ç›¸å…³åŠ åˆ†
        for keyword in self.question_keywords:
            if keyword in content:
                score += 2.0
                break
        
        # å›ç­”ç›¸å…³åŠ åˆ† 
        for keyword in self.answer_keywords:
            if keyword in content:
                score += 1.5
                break
        
        # æŸ¯å—ç›¸å…³åŠ åˆ†
        conan_matches = sum(1 for keyword in self.conan_keywords if keyword in content)
        score += conan_matches * 1.0
        
        # æœ‰ç”¨ä¿¡æ¯åŠ åˆ†
        useful_matches = sum(1 for keyword in self.useful_keywords if keyword in content)
        score += useful_matches * 0.8
        
        # URLé“¾æ¥åŠ åˆ†
        if "http" in content or "www." in content:
            score += 1.0
        
        # å…·ä½“æ•°å­—ä¿¡æ¯åŠ åˆ†ï¼ˆé›†æ•°ã€ç« èŠ‚ç­‰ï¼‰
        if re.search(r'\d+é›†|\d+è¯|\d+å·|\d+ç« ', content):
            score += 0.8
        
        return score
    
    def should_keep_post(self, post: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿ç•™è¿™ä¸ªæ¥¼å±‚"""
        content = post['content']
        
        # æ£€æŸ¥æ˜¯å¦è¢«æŠ˜å 
        if self.is_system_folded(content):
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ— æ„ä¹‰
        if self.is_meaningless_short(content):
            return False
        
        # æ ¹æ®åˆ†æ•°åˆ¤æ–­
        score = self.calculate_content_score(content)
        return score >= 1.5  # ä¿ç•™ä¸­ç­‰ä»·å€¼ä»¥ä¸Šçš„å†…å®¹

class TiebaPostCrawler:
    def __init__(self, post_id: str, max_pages: int = 2):
        self.post_id = post_id
        self.max_pages = max_pages
        self.base_url = f"https://tieba.baidu.com/p/{post_id}"
        self.session = requests.Session()
        
        # æ›´å¼ºçš„ååçˆ¬è™«è¯·æ±‚å¤´æ± 
        self.headers_pool = [
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            },
            {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh-Hans;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            },
            {
                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
        ]
        
        self.posts = []  # å­˜å‚¨æ‰€æœ‰æ¥¼å±‚æ•°æ®
        self.filtered_posts = []  # å­˜å‚¨è¿‡æ»¤åçš„é«˜ä»·å€¼æ•°æ®
        self.post_title = ""
        self.filter = TiebaQAFilter()
        self.captcha_count = 0  # éªŒè¯ç é‡åˆ°æ¬¡æ•°
        
    def get_random_headers(self):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        return random.choice(self.headers_pool)
        
    def fetch_page(self, page_num: int, max_retries: int = 3) -> Optional[BeautifulSoup]:
        """è·å–æŒ‡å®šé¡µé¢çš„HTMLå†…å®¹ï¼Œå¸¦ååçˆ¬è™«ç­–ç•¥"""
        url = f"{self.base_url}?pn={page_num}"
        
        for attempt in range(max_retries):
            try:
                # éšæœºå»¶è¿Ÿ
                delay = random.uniform(5, 12)  # 5-12ç§’éšæœºå»¶è¿Ÿ
                if attempt > 0:
                    print(f"â³ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                
                # ä½¿ç”¨éšæœºè¯·æ±‚å¤´
                headers = self.get_random_headers()
                
                print(f"ğŸ“¥ æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ (å°è¯• {attempt + 1}/{max_retries}): {url}")
                print(f"ğŸ”§ ä½¿ç”¨UA: {headers['User-Agent'][:50]}...")
                
                response = self.session.get(url, headers=headers, timeout=20)
                response.raise_for_status()
                
                print(f"âœ… è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
                print(f"ğŸ“„ å“åº”é•¿åº¦: {len(response.text)} å­—ç¬¦")
                
                # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘æˆ–éœ€è¦éªŒè¯
                if "éªŒè¯" in response.text or "captcha" in response.url.lower() or "verify" in response.text.lower():
                    self.captcha_count += 1
                    print(f"ğŸš« ç¬¬ {page_num} é¡µé‡åˆ°éªŒè¯ç  (ç¬¬{self.captcha_count}æ¬¡)")
                    
                    if self.captcha_count >= 3:
                        print("âŒ éªŒè¯ç æ¬¡æ•°è¿‡å¤šï¼Œå»ºè®®ä½¿ç”¨æ‰‹åŠ¨æ–¹æ¡ˆ")
                        print("ğŸ’¡ è¯·è¿è¡Œ: python scripts/anti_captcha_guide.py")
                        return None
                    
                    # å¢åŠ æ›´é•¿å»¶è¿Ÿå†é‡è¯•
                    longer_delay = random.uniform(15, 25)
                    print(f"â³ é‡åˆ°éªŒè¯ç ï¼Œç­‰å¾… {longer_delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(longer_delay)
                    continue
                
                # æ£€æŸ¥é¡µé¢å†…å®¹æ˜¯å¦æ­£å¸¸
                if len(response.text) < 1000:
                    print(f"âš ï¸  ç¬¬ {page_num} é¡µå†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½è¢«é™åˆ¶")
                    continue
                
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                return soup
                
            except requests.Timeout as e:
                print(f"â° ç¬¬ {page_num} é¡µè¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    wait_time = random.uniform(10, 20)
                    print(f"â³ ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                
            except requests.RequestException as e:
                print(f"âŒ ç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(5, 10))
        
        print(f"ğŸ’” ç¬¬ {page_num} é¡µç»è¿‡ {max_retries} æ¬¡å°è¯•ä»ç„¶å¤±è´¥")
        return None
    
    def extract_title(self, soup: BeautifulSoup) -> str:
        """æå–å¸–å­æ ‡é¢˜"""
        # å°è¯•å¤šç§å¯èƒ½çš„æ ‡é¢˜é€‰æ‹©å™¨
        title_selectors = [
            'h3.core_title_txt',
            '.core_title_txt',
            'h1',
            'h2', 
            '.thread_title',
            'title'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                if title and "ç™¾åº¦è´´å§" not in title:
                    return title
        
        return "æœªçŸ¥æ ‡é¢˜"
    
    def extract_posts_from_page(self, soup: BeautifulSoup, page_num: int) -> List[Dict]:
        """ä»é¡µé¢ä¸­æå–æ‰€æœ‰æ¥¼å±‚å†…å®¹"""
        posts = []
        
        # å°è¯•å¤šç§å¯èƒ½çš„æ¥¼å±‚å®¹å™¨é€‰æ‹©å™¨
        post_selectors = [
            '.l_post',
            '.j_l_post', 
            '.core_reply',
            '.p_postlist .l_post',
            'div[data-field]'  # è´´å§æ¥¼å±‚é€šå¸¸æœ‰data-fieldå±æ€§
        ]
        
        post_elements = []
        for selector in post_selectors:
            post_elements = soup.select(selector)
            if post_elements:
                print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(post_elements)} ä¸ªæ¥¼å±‚")
                break
        
        if not post_elements:
            print(f"âš ï¸  ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°æ¥¼å±‚å†…å®¹ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾åŒ…å«ç”¨æˆ·å‘è¨€çš„div
            post_elements = soup.find_all('div', class_=re.compile(r'post|reply'))
        
        for i, post_elem in enumerate(post_elements):
            try:
                post_data = self.extract_single_post(post_elem, page_num, i)
                if post_data and post_data['content'].strip():
                    posts.append(post_data)
            except Exception as e:
                print(f"âš ï¸  è§£æç¬¬ {page_num} é¡µç¬¬ {i+1} ä¸ªæ¥¼å±‚æ—¶å‡ºé”™: {e}")
                continue
        
        return posts
    
    def extract_single_post(self, post_elem, page_num: int, index: int) -> Optional[Dict]:
        """æå–å•ä¸ªæ¥¼å±‚çš„è¯¦ç»†ä¿¡æ¯"""
        try:
            # æå–æ¥¼å±‚å·
            floor_num = None
            floor_selectors = [
                '.tail-info .tail-info-num',
                '.floor',
                '.post-tail-wrap .tail-info',
                '[data-field]'  # æœ‰æ—¶æ¥¼å±‚å·åœ¨data-fieldä¸­
            ]
            
            for selector in floor_selectors:
                floor_elem = post_elem.select_one(selector)
                if floor_elem:
                    floor_text = floor_elem.get_text(strip=True)
                    floor_match = re.search(r'(\d+)æ¥¼', floor_text)
                    if floor_match:
                        floor_num = int(floor_match.group(1))
                        break
            
            # å¦‚æœæ²¡æ‰¾åˆ°æ¥¼å±‚å·ï¼Œä½¿ç”¨é¡µé¢å†…ç´¢å¼•ä¼°ç®—
            if floor_num is None:
                # æ¯é¡µå¤§çº¦30æ¥¼ï¼Œæ ¹æ®é¡µç å’Œç´¢å¼•ä¼°ç®—
                floor_num = (page_num - 1) * 30 + index + 1
            
            # æå–ç”¨æˆ·å
            username = "åŒ¿åç”¨æˆ·"
            username_selectors = [
                '.j_user_card',
                '.p_author_name',
                '.username',
                '.author'
            ]
            
            for selector in username_selectors:
                username_elem = post_elem.select_one(selector)
                if username_elem:
                    username = username_elem.get_text(strip=True)
                    break
            
            # æå–å‘å¸–æ—¶é—´
            post_time = ""
            time_selectors = [
                '.tail-info:last-child',
                '.post-tail-wrap .tail-info:last-child',
                '.j_reply_data',
                '.post-time'
            ]
            
            for selector in time_selectors:
                time_elem = post_elem.select_one(selector)
                if time_elem:
                    post_time = time_elem.get_text(strip=True)
                    break
            
            # æå–å¸–å­å†…å®¹
            content = ""
            content_selectors = [
                '.d_post_content',
                '.j_d_post_content',
                '.post-content',
                '.content',
                '.cc .j_d_post_content'
            ]
            
            for selector in content_selectors:
                content_elem = post_elem.select_one(selector)
                if content_elem:
                    # ç§»é™¤å¹¿å‘Šå’Œæ— å…³å…ƒç´ 
                    for ad in content_elem.select('.j_sponsor, .at, .emotion'):
                        ad.decompose()
                    
                    content = content_elem.get_text(separator='\n', strip=True)
                    break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•æ›´å®½æ³›çš„æœç´¢
            if not content:
                content = post_elem.get_text(separator='\n', strip=True)
                # æ¸…ç†æ‰æ˜æ˜¾çš„å¯¼èˆªå’Œå¹¿å‘Šæ–‡æœ¬
                content = re.sub(r'(å›å¤|åˆ é™¤|ä¸¾æŠ¥|åˆ†äº«|æ”¶è—|ç­¾åˆ°|ç­‰çº§|ç»éªŒ)', '', content)
                content = re.sub(r'\s+', ' ', content).strip()
            
            return {
                'floor': floor_num,
                'username': username,
                'post_time': post_time,
                'content': content,
                'page': page_num
            }
            
        except Exception as e:
            print(f"âŒ è§£ææ¥¼å±‚å‡ºé”™: {e}")
            return None
    
    def crawl_all_pages(self):
        """çˆ¬å–æ‰€æœ‰é¡µé¢"""
        print(f"ğŸš€ å¼€å§‹æŠ“å–å¸–å­ {self.post_id}ï¼Œå…± {self.max_pages} é¡µ")
        
        for page_num in range(1, self.max_pages + 1):
            soup = self.fetch_page(page_num)
            if soup is None:
                print(f"âš ï¸  è·³è¿‡ç¬¬ {page_num} é¡µ")
                continue
            
            # ç¬¬ä¸€é¡µæ—¶æå–æ ‡é¢˜
            if page_num == 1 and not self.post_title:
                self.post_title = self.extract_title(soup)
                print(f"ğŸ“‹ å¸–å­æ ‡é¢˜: {self.post_title}")
            
            # æå–æ¥¼å±‚å†…å®¹
            page_posts = self.extract_posts_from_page(soup, page_num)
            self.posts.extend(page_posts)
            
            print(f"âœ… ç¬¬ {page_num} é¡µæŠ“å–å®Œæˆï¼Œè·å¾— {len(page_posts)} ä¸ªæ¥¼å±‚")
            
            # å»¶è¿Ÿé¿å…è¢«ban
            time.sleep(2)
        
        print(f"ğŸ‰ æŠ“å–å®Œæˆï¼æ€»å…±è·å¾— {len(self.posts)} ä¸ªæ¥¼å±‚")
        
        # æ‰§è¡Œæ™ºèƒ½è¿‡æ»¤
        self.apply_filter()
    
    def apply_filter(self):
        """åº”ç”¨æ™ºèƒ½è¿‡æ»¤å™¨"""
        print(f"\nğŸ§¹ å¼€å§‹æ™ºèƒ½è¿‡æ»¤...")
        
        filtered_count = 0
        for post in self.posts:
            if self.filter.should_keep_post(post):
                # æ·»åŠ è¯„åˆ†ä¿¡æ¯
                post['score'] = self.filter.calculate_content_score(post['content'])
                self.filtered_posts.append(post)
                filtered_count += 1
        
        removed_count = len(self.posts) - filtered_count
        print(f"âœ… è¿‡æ»¤å®Œæˆï¼šä¿ç•™ {filtered_count} ä¸ªé«˜ä»·å€¼æ¥¼å±‚ï¼Œè¿‡æ»¤æ‰ {removed_count} ä¸ªä½è´¨é‡å†…å®¹")
        
        if removed_count > 0:
            print(f"ğŸ“Š ä¿ç•™ç‡: {filtered_count/len(self.posts)*100:.1f}%")
    
    def save_to_file(self, save_json: bool = False):
        """ä¿å­˜æŠ“å–ç»“æœåˆ°æ–‡ä»¶"""
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = "data/interviews"
        os.makedirs(output_dir, exist_ok=True)
        
        # æ¸…ç†æ ‡é¢˜ç”¨ä½œæ–‡ä»¶å
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', self.post_title)
        safe_title = safe_title[:50]  # é™åˆ¶é•¿åº¦
        
        # ä¿å­˜è¿‡æ»¤åçš„é«˜ä»·å€¼å†…å®¹ä¸ºæ–‡æœ¬æ–‡ä»¶ï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰
        txt_filename = f"{output_dir}/tieba_{self.post_id}_{safe_title}.txt"
        with open(txt_filename, 'w', encoding='utf-8') as f:
            f.write(f"{self.post_title}\n\n")
            f.write(f"å¸–å­ID: {self.post_id}\n")
            f.write(f"æŠ“å–æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"åŸå§‹æ¥¼å±‚æ•°: {len(self.posts)}\n")
            f.write(f"é«˜ä»·å€¼æ¥¼å±‚æ•°: {len(self.filtered_posts)}\n")
            f.write("=" * 50 + "\n\n")
            
            for post in self.filtered_posts:
                f.write(f"ã€{post['floor']}æ¥¼ã€‘{post['username']}\n")
                if post['post_time']:
                    f.write(f"æ—¶é—´: {post['post_time']}\n")
                f.write(f"\n{post['content']}\n\n")
        
        print(f"ğŸ’¾ è¿‡æ»¤åå†…å®¹å·²ä¿å­˜åˆ°: {txt_filename}")
        
        # å¯é€‰ä¿å­˜JSONæ–‡ä»¶ï¼ˆå®Œæ•´æ•°æ®ï¼‰
        if save_json:
            json_filename = f"{output_dir}/tieba_{self.post_id}_{safe_title}.json"
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'title': self.post_title,
                    'post_id': self.post_id,
                    'crawl_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'total_posts': len(self.posts),
                    'filtered_posts': len(self.filtered_posts),
                    'raw_posts': self.posts,
                    'filtered_posts_data': self.filtered_posts
                }, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å®Œæ•´æ•°æ®å·²ä¿å­˜åˆ°: {json_filename}")
        
        return txt_filename
    
    def run(self, save_json: bool = False):
        """è¿è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹"""
        self.crawl_all_pages()
        result_file = self.save_to_file(save_json)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æŠ“å–ç»Ÿè®¡:")
        print(f"   æ ‡é¢˜: {self.post_title}")
        print(f"   åŸå§‹æ¥¼å±‚: {len(self.posts)}")
        print(f"   é«˜ä»·å€¼æ¥¼å±‚: {len(self.filtered_posts)}")
        if self.filtered_posts:
            print(f"   æ¥¼å±‚èŒƒå›´: {min(p['floor'] for p in self.filtered_posts)} - {max(p['floor'] for p in self.filtered_posts)}")
            avg_score = sum(p['score'] for p in self.filtered_posts) / len(self.filtered_posts)
            print(f"   å¹³å‡è¯„åˆ†: {avg_score:.1f}")
        
        return result_file

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ•·ï¸  ç™¾åº¦è´´å§æ™ºèƒ½çˆ¬è™« v2.0")
    print("=" * 50)
    
    # é…ç½®å‚æ•°
    POST_ID = "7223841891"
    MAX_PAGES = 10  # æµ‹è¯•é˜¶æ®µåªæŠ“å–å‰2é¡µ
    
    print(f"ğŸ“‹ æµ‹è¯•æ¨¡å¼ï¼šåªæŠ“å–å‰ {MAX_PAGES} é¡µï¼Œè‡ªåŠ¨è¿‡æ»¤ä½è´¨é‡å†…å®¹")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶è¿è¡Œ
    crawler = TiebaPostCrawler(POST_ID, MAX_PAGES)
    
    # å…ˆæµ‹è¯•ç½‘ç»œè¿æ¥
    print("\nğŸ” æµ‹è¯•ç½‘ç»œè¿æ¥...")
    try:
        test_response = requests.get("https://www.baidu.com", timeout=5)
        print(f"âœ… ç™¾åº¦é¦–é¡µè®¿é—®æ­£å¸¸: {test_response.status_code}")
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¿æ¥å¼‚å¸¸: {e}")
        return
    
    # è¿è¡Œçˆ¬è™«ï¼ˆé»˜è®¤ä¸ç”ŸæˆJSONï¼‰
    result_file = crawler.run(save_json=False)
    print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {result_file}")

def test_single_page():
    """æµ‹è¯•å•é¡µæŠ“å–åŠŸèƒ½"""
    print("ğŸ§ª å•é¡µæµ‹è¯•æ¨¡å¼")
    crawler = TiebaPostCrawler("7223841891", 1)
    soup = crawler.fetch_page(1)
    
    if soup:
        print("âœ… æˆåŠŸè·å–é¡µé¢å†…å®¹")
        print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")
        
        # å°è¯•æå–åŸºæœ¬ä¿¡æ¯
        title = crawler.extract_title(soup)
        print(f"ğŸ“‹ æå–çš„æ ‡é¢˜: {title}")
        
    else:
        print("âŒ æ— æ³•è·å–é¡µé¢å†…å®¹")

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_single_page()
    else:
        main()